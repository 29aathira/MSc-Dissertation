{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgTWb2bmatt/CmwJbS7bsu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/29aathira/MSc-Dissertation/blob/main/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BxxvQaTQaqY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df.values\n",
        "\n",
        "data = load_data('/content/sales_train_evaluation.csv')\n",
        "\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import mse, binary_crossentropy\n",
        "\n",
        "# Define input dimension\n",
        "input_dim = 1947\n",
        "\n",
        "# Define latent space dimension\n",
        "latent_dim = 10  # You can adjust this based on your requirements\n",
        "\n",
        "# Encoder network\n",
        "inputs = Input(shape=(input_dim,))\n",
        "enc = Dense(512, activation='relu')(inputs)\n",
        "enc_mean = Dense(latent_dim)(enc)\n",
        "enc_log_var = Dense(latent_dim)(enc)\n",
        "\n",
        "# Reparameterization trick\n",
        "def sampling(args):\n",
        "    enc_mean, enc_log_var = args\n",
        "    epsilon = tf.random.normal(shape=(tf.shape(enc_mean)[0], latent_dim))\n",
        "    return enc_mean + tf.exp(0.5 * enc_log_var) * epsilon\n",
        "\n",
        "latent_vector = Lambda(sampling)([enc_mean, enc_log_var])\n",
        "\n",
        "# Decoder network\n",
        "dec = Dense(512, activation='relu')(latent_vector)\n",
        "outputs = Dense(input_dim)(dec)\n",
        "\n",
        "# VAE model\n",
        "vae = Model(inputs, outputs)\n",
        "\n",
        "# Define VAE loss function\n",
        "def vae_loss(inputs, outputs):\n",
        "    reconstruction_loss = mse(inputs, outputs)\n",
        "    kl_loss = -0.5 * tf.reduce_mean(1 + enc_log_var - tf.square(enc_mean) - tf.exp(enc_log_var))\n",
        "    return reconstruction_loss + kl_loss\n",
        "\n",
        "vae.compile(optimizer='adam', loss=vae_loss)\n",
        "\n",
        "# Train VAE\n",
        "vae.fit(train_data, train_data, epochs=50, batch_size=32, validation_data=(val_data, val_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "p-Fe7QbNQtZO",
        "outputId": "7b2f2fd4-3811-4f39-ce1c-6f0749b75eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-dadb329fe15e>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Train VAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "# Load data\n",
        "def load_data():\n",
        "    df = pd.read_csv('/content/sales_train_evaluation.csv')\n",
        "    return df.values\n",
        "\n",
        "data = load_data()"
      ],
      "metadata": {
        "id": "XVXJEFYoRO6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import mse, KLD\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Load data\n",
        "def load_data():\n",
        "    df = pd.read_csv('/content/sales_train_evaluation.csv')\n",
        "    return df.values\n",
        "\n",
        "data = load_data()\n",
        "\n",
        "# Convert string values to integers\n",
        "def preprocess_data(data):\n",
        "    label_encoders = []\n",
        "    for col_idx in range(data.shape[1]):\n",
        "        if isinstance(data[0, col_idx], str):\n",
        "            le = LabelEncoder()\n",
        "            data[:, col_idx] = le.fit_transform(data[:, col_idx])\n",
        "            label_encoders.append(le)\n",
        "    return data, label_encoders\n",
        "\n",
        "data, label_encoders = preprocess_data(data)\n",
        "\n",
        "# Split data into train and validation sets\n",
        "def split_data(data):\n",
        "    train_size = int(0.8 * len(data))\n",
        "    train_data = data[:train_size]\n",
        "    val_data = data[train_size:]\n",
        "    return train_data, val_data\n",
        "\n",
        "train_data, val_data = split_data(data)\n",
        "\n",
        "# Convert data to TensorFlow tensors\n",
        "train_data_tf = tf.convert_to_tensor(train_data, dtype=tf.float32)\n",
        "val_data_tf = tf.convert_to_tensor(val_data, dtype=tf.float32)\n",
        "\n",
        "# Multivariate VAE\n",
        "latent_dim = 2  # Latent dimension\n",
        "\n",
        "# Encoder\n",
        "input_shape = train_data.shape[1]\n",
        "inputs = Input(shape=(input_shape,))\n",
        "x = Dense(64, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim)(x)\n",
        "z_log_var = Dense(latent_dim)(x)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), dtype=tf.float32)\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "# Decoder\n",
        "latent_inputs = Input(shape=(latent_dim,))\n",
        "x = Dense(64, activation='relu')(latent_inputs)\n",
        "outputs = Dense(input_shape, activation='linear')(x)\n",
        "\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "\n",
        "# VAE\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae')\n",
        "\n",
        "# Loss\n",
        "reconstruction_loss = mse(inputs, outputs)\n",
        "kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# Train the VAE\n",
        "vae.fit(train_data_tf, epochs=50, batch_size=32, validation_data=(val_data_tf,))\n",
        "\n",
        "# Generate augmented data\n",
        "def generate_augmented_data(data, num_samples=100):\n",
        "    latent_samples = np.random.normal(size=(num_samples, latent_dim))\n",
        "    latent_samples_tf = tf.convert_to_tensor(latent_samples, dtype=tf.float32)\n",
        "    generated_data = decoder.predict(latent_samples_tf)\n",
        "\n",
        "    # Check for NaN values\n",
        "    if np.any(np.isnan(generated_data)):\n",
        "        # Remove NaN values\n",
        "        generated_data = generated_data[~np.isnan(generated_data)]\n",
        "\n",
        "    return generated_data\n",
        "\n",
        "augmented_data = generate_augmented_data(train_data, num_samples=1000)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5pSfkVmRXB0",
        "outputId": "48b255d7-fcb0-45f8-c37b-3189d395034a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "763/763 [==============================] - 8s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/50\n",
            "763/763 [==============================] - 6s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/50\n",
            "763/763 [==============================] - 6s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/50\n",
            "763/763 [==============================] - 6s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 11/50\n",
            "763/763 [==============================] - 7s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 12/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 13/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 14/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 15/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 16/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 17/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 18/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 19/50\n",
            "763/763 [==============================] - 6s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 20/50\n",
            "763/763 [==============================] - 7s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 21/50\n",
            "763/763 [==============================] - 6s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 22/50\n",
            "763/763 [==============================] - 7s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 23/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 24/50\n",
            "763/763 [==============================] - 7s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 25/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 26/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 27/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 28/50\n",
            "763/763 [==============================] - 6s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 29/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 30/50\n",
            "763/763 [==============================] - 6s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 31/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 32/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 33/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 34/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 35/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 36/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 37/50\n",
            "763/763 [==============================] - 6s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 38/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 39/50\n",
            "763/763 [==============================] - 6s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 40/50\n",
            "763/763 [==============================] - 7s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 41/50\n",
            "763/763 [==============================] - 6s 8ms/step - loss: nan - val_loss: nan\n",
            "Epoch 42/50\n",
            "763/763 [==============================] - 7s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 43/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 44/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 45/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 46/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 47/50\n",
            "763/763 [==============================] - 8s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 48/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "Epoch 49/50\n",
            "763/763 [==============================] - 7s 10ms/step - loss: nan - val_loss: nan\n",
            "Epoch 50/50\n",
            "763/763 [==============================] - 7s 9ms/step - loss: nan - val_loss: nan\n",
            "32/32 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_data = generate_augmented_data(train_data, num_samples=1000)\n"
      ],
      "metadata": {
        "id": "09biNl2lmRYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccee3fb8-01a6-4865-c2c4-215e8e33900e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QszwGsWdvZya"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}